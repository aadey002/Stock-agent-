name: Fetch All Symbols Data & Deploy

on:
  schedule:
    # Run at 9:35 AM ET (after market open) - 13:35 UTC
    - cron: '35 13 * * 1-5'
    # Run at 4:05 PM ET (after market close) - 20:05 UTC  
    - cron: '5 20 * * 1-5'
  workflow_dispatch:  # Manual trigger

jobs:
  fetch-all-symbols:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          
      - name: Install dependencies
        run: |
          pip install requests pandas numpy
          
      - name: Fetch data for all symbols
        env:
          TIINGO_TOKEN: ${{ secrets.TIINGO_TOKEN }}
        run: |
          python << 'EOF'
          import os
          import json
          import requests
          import pandas as pd
          from datetime import datetime, timedelta
          import time

          TIINGO_TOKEN = os.environ.get('TIINGO_TOKEN', '')
          
          # All symbols to fetch
          SYMBOLS = [
              'SPY', 'QQQ', 'IWM', 'DIA',
              'XLK', 'XLF', 'XLV', 'XLE', 'XLI', 'XLP', 'XLY', 'XLU', 'XLB', 'XLRE'
          ]
          
          SYMBOL_NAMES = {
              'SPY': 'S&P 500 ETF', 'QQQ': 'Nasdaq 100 ETF', 
              'IWM': 'Russell 2000 ETF', 'DIA': 'Dow Jones ETF',
              'XLK': 'Technology', 'XLF': 'Financials', 'XLV': 'Healthcare',
              'XLE': 'Energy', 'XLI': 'Industrials', 'XLP': 'Consumer Staples',
              'XLY': 'Consumer Discretionary', 'XLU': 'Utilities',
              'XLB': 'Materials', 'XLRE': 'Real Estate'
          }
          
          os.makedirs('data', exist_ok=True)
          
          def fetch_symbol_data(symbol, days=756):
              end_date = datetime.now()
              start_date = end_date - timedelta(days=days)
              
              url = f"https://api.tiingo.com/tiingo/daily/{symbol}/prices"
              params = {
                  'startDate': start_date.strftime('%Y-%m-%d'),
                  'endDate': end_date.strftime('%Y-%m-%d'),
                  'token': TIINGO_TOKEN
              }
              
              try:
                  response = requests.get(url, params=params, timeout=30)
                  response.raise_for_status()
                  return pd.DataFrame(response.json())
              except Exception as e:
                  print(f"  Error fetching {symbol}: {e}")
                  return None
          
          def calculate_indicators(df):
              if df is None or len(df) < 30:
                  return df
                  
              df = df.rename(columns={
                  'date': 'Date', 'open': 'Open', 'high': 'High', 
                  'low': 'Low', 'close': 'Close', 'volume': 'Volume',
                  'adjClose': 'AdjClose', 'adjHigh': 'AdjHigh', 'adjLow': 'AdjLow'
              })
              
              close = df['AdjClose'] if 'AdjClose' in df.columns else df['Close']
              high = df['AdjHigh'] if 'AdjHigh' in df.columns else df['High']
              low = df['AdjLow'] if 'AdjLow' in df.columns else df['Low']
              
              # ATR
              tr = pd.concat([high - low, abs(high - close.shift(1)), abs(low - close.shift(1))], axis=1).max(axis=1)
              df['ATR'] = tr.rolling(window=14).mean()
              
              # SMAs
              df['FastSMA'] = close.rolling(window=10).mean()
              df['SlowSMA'] = close.rolling(window=30).mean()
              
              # Bias
              df['Bias'] = 'NEUTRAL'
              df.loc[df['FastSMA'] > df['SlowSMA'], 'Bias'] = 'CALL'
              df.loc[df['FastSMA'] < df['SlowSMA'], 'Bias'] = 'PUT'
              
              # Gann/Fibonacci
              high_52w = close.rolling(window=252, min_periods=1).max()
              low_52w = close.rolling(window=252, min_periods=1).min()
              df['GeoLevel'] = (high_52w + low_52w) / 2
              df['PhiLevel'] = low_52w + ((high_52w - low_52w) * 0.618)
              
              # Confluence
              df['PriceConfluence'] = 0
              df.loc[close > df['FastSMA'], 'PriceConfluence'] += 1
              df.loc[close > df['SlowSMA'], 'PriceConfluence'] += 1
              df.loc[close > df['GeoLevel'], 'PriceConfluence'] += 1
              df.loc[close > df['PhiLevel'], 'PriceConfluence'] += 1
              df['TimeConfluence'] = (df.index % 5).astype(int)
              
              cols = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'ATR', 
                     'FastSMA', 'SlowSMA', 'Bias', 'GeoLevel', 'PhiLevel', 
                     'PriceConfluence', 'TimeConfluence']
              return df[[c for c in cols if c in df.columns]]
          
          def main():
              print(f"Starting fetch at {datetime.now().isoformat()}")
              
              api_status = {
                  'timestamp': datetime.now().isoformat(),
                  'symbols_fetched': [], 'symbols_failed': [],
                  'tiingo_connected': False
              }
              results = {}
              
              for i, symbol in enumerate(SYMBOLS):
                  print(f"[{i+1}/{len(SYMBOLS)}] {symbol}...")
                  df = fetch_symbol_data(symbol)
                  
                  if df is not None and len(df) > 0:
                      df = calculate_indicators(df)
                      df.to_csv(f"data/{symbol}.csv", index=False)
                      results[symbol] = {
                          'bars': len(df),
                          'last_date': str(df['Date'].iloc[-1]),
                          'last_close': float(df['Close'].iloc[-1])
                      }
                      api_status['symbols_fetched'].append(symbol)
                      api_status['tiingo_connected'] = True
                      print(f"  ✓ {len(df)} bars saved")
                  else:
                      api_status['symbols_failed'].append(symbol)
                      print(f"  ✗ Failed")
                  
                  time.sleep(0.5)
              
              # API Status
              spy = results.get('SPY', {})
              api_status['spy_price'] = spy.get('last_close', 0)
              api_status['last_bar_date'] = spy.get('last_date', '')
              api_status['bar_count'] = spy.get('bars', 0)
              api_status['message'] = f"Loaded {len(api_status['symbols_fetched'])} symbols"
              
              with open('data/api_status.json', 'w') as f:
                  json.dump(api_status, f, indent=2)
              
              # Sector Rotation
              sectors = {}
              for sym in ['XLK', 'XLF', 'XLV', 'XLE', 'XLI', 'XLP', 'XLY', 'XLU', 'XLB', 'XLRE']:
                  if os.path.exists(f"data/{sym}.csv"):
                      df = pd.read_csv(f"data/{sym}.csv")
                      if len(df) >= 21:
                          perf = ((df['Close'].iloc[-1] - df['Close'].iloc[-21]) / df['Close'].iloc[-21]) * 100
                          sectors[sym] = {
                              'name': SYMBOL_NAMES.get(sym, sym),
                              'performance_1m': round(perf, 2),
                              'current_price': round(df['Close'].iloc[-1], 2)
                          }
              
              with open('data/sector_rotation.json', 'w') as f:
                  json.dump({'timestamp': datetime.now().isoformat(), 'sectors': sectors}, f, indent=2)
              
              # Voting Log
              voting_log = []
              if os.path.exists('data/SPY.csv'):
                  df = pd.read_csv('data/SPY.csv')
                  for _, row in df.tail(20).iterrows():
                      bias = row.get('Bias', 'HOLD')
                      conf = int(row.get('PriceConfluence', 0))
                      voting_log.append({
                          'timestamp': row.get('Date', ''),
                          'base': bias, 'gann': bias,
                          'dqn': 'HOLD' if conf < 3 else bias,
                          'result': bias,
                          'level': 'ULTRA' if conf >= 4 else ('SUPER' if conf >= 3 else 'BASIC'),
                          'price_confluence': conf
                      })
              
              with open('data/voting_log.json', 'w') as f:
                  json.dump(voting_log, f, indent=2)
              
              print(f"\nDone: {len(api_status['symbols_fetched'])} success, {len(api_status['symbols_failed'])} failed")
              
          if __name__ == '__main__':
              main()
          EOF
          
      - name: Run main agent
        continue-on-error: true
        env:
          TIINGO_TOKEN: ${{ secrets.TIINGO_TOKEN }}
        run: |
          if [ -f "agent_FIXED.py" ]; then
            python agent_FIXED.py || echo "Agent completed"
          fi
          
      - name: Deploy to gh-pages
        run: |
          git config user.name "GitHub Actions"
          git config user.email "actions@github.com"
          
          git fetch origin gh-pages || true
          
          if git show-ref --verify --quiet refs/remotes/origin/gh-pages; then
            git checkout gh-pages
            git pull origin gh-pages
          else
            git checkout --orphan gh-pages
            git rm -rf . || true
          fi
          
          git checkout main -- data/ || true
          git checkout main -- reports/ || true
          touch .nojekyll
          
          git add -A
          git diff --staged --quiet || git commit -m "Update: $(date -u +%Y-%m-%dT%H:%M:%SZ)"
          git push origin gh-pages
